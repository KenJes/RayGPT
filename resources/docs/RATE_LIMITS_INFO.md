# âš ï¸ Rate Limits de GitHub Models (Free Tier)

## ğŸš¨ Error 429: RateLimitReached

Si ves este error, has alcanzado los lÃ­mites del **Free Tier** de GitHub Models.

## ğŸ“Š LÃ­mites del Free Tier

GitHub Models ofrece acceso gratuito con los siguientes lÃ­mites:

| LÃ­mite | Valor | DescripciÃ³n |
|--------|-------|-------------|
| **RPM** | 15 | Requests por minuto |
| **RPD** | 150 | Requests por dÃ­a |
| **TPM** | 150,000 | Tokens por minuto |

## ğŸ”„ SoluciÃ³n AutomÃ¡tica

**Raymundo tiene fallback automÃ¡tico implementado:**

Cuando se alcanza el rate limit:
1. âœ… Detecta el error 429 automÃ¡ticamente
2. ğŸ”„ Cambia a **Ollama** (modelo local, sin lÃ­mites)
3. ğŸ“¢ Te notifica con el mensaje: `âš ï¸ *[Modo local - Rate limit alcanzado]*`
4. ğŸš€ ContinÃºa funcionando sin interrupciones

**Ejemplo de respuesta con fallback:**
```
âš ï¸ *[Modo local - Rate limit alcanzado]*

Claro, puedo ayudarte con eso...
```

## â° Â¿CuÃ¡ndo se reinician los lÃ­mites?

- **RPM (15 por minuto):** Se reinicia cada minuto
- **RPD (150 por dÃ­a):** Se reinicia a las 00:00 UTC
- **TPM (150K por minuto):** Se reinicia cada minuto

## ğŸ“‰ Reducir el Consumo

### 1. Usar Ollama directamente para tareas simples

Modifica el cÃ³digo para usar solo Ollama en chats casuales:

```python
# En whatsapp_server.py
if 'simple' in mensaje.lower():
    # Usar solo Ollama
    respuesta = gestor.ollama.generate(mensaje)
else:
    # Usar hÃ­brido (GPT-4o + fallback)
    respuesta = gestor.chat_hibrido(mensaje)
```

### 2. Implementar cachÃ© de respuestas

```python
# Cache para preguntas repetidas
cache_respuestas = {}

def get_cached_or_generate(mensaje):
    if mensaje in cache_respuestas:
        return cache_respuestas[mensaje]
    
    respuesta = gestor.chat_hibrido(mensaje)
    cache_respuestas[mensaje] = respuesta
    return respuesta
```

### 3. Limitar requests por usuario

```python
# En whatsapp_server.py
usuarios_requests = {}
MAX_REQUESTS_POR_HORA = 10

def check_user_limits(user_id):
    now = datetime.now()
    if user_id not in usuarios_requests:
        usuarios_requests[user_id] = []
    
    # Filtrar requests de Ãºltima hora
    usuarios_requests[user_id] = [
        t for t in usuarios_requests[user_id] 
        if now - t < timedelta(hours=1)
    ]
    
    if len(usuarios_requests[user_id]) >= MAX_REQUESTS_POR_HORA:
        return False
    
    usuarios_requests[user_id].append(now)
    return True
```

### 4. Priorizar Ollama para funciones especÃ­ficas

```python
# Correcciones ortogrÃ¡ficas â†’ Solo Ollama (rÃ¡pido, sin consumir GPT-4o)
# Web scraping â†’ GPT-4o (mejor anÃ¡lisis)
# Documentos/Presentaciones â†’ GPT-4o (mejor calidad)
# Chat casual â†’ Ollama (suficiente para conversaciÃ³n)
```

## ğŸ¯ Estrategia Recomendada

```python
def elegir_modelo(tipo_tarea, complejidad):
    """
    Elige el modelo Ã³ptimo segÃºn la tarea
    """
    if tipo_tarea in ['chat_casual', 'correccion', 'optimizacion']:
        # Tareas simples â†’ Ollama
        return 'ollama'
    
    elif tipo_tarea in ['documento', 'presentacion', 'hoja_calculo']:
        # GeneraciÃ³n de contenido â†’ GPT-4o
        return 'gpt4o'
    
    elif tipo_tarea == 'web_scraping':
        # AnÃ¡lisis complejo â†’ GPT-4o
        return 'gpt4o'
    
    else:
        # Por defecto, hÃ­brido con fallback
        return 'hibrido'
```

## ğŸ’¡ Verificar Estado de LÃ­mites

```bash
# Ver estadÃ­sticas actuales
curl "http://localhost:5000/stats?format=text"
```

O desde WhatsApp:
```
/raymundo stats
```

**Salida esperada:**
```
ğŸ“Š Rate Limits (GitHub Models Free):
â€¢ Tokens/Minuto: 145,230 / 150,000 TPM (96.8%)  âš ï¸ CERCA DEL LÃMITE
â€¢ Requests/DÃ­a: 142 / 150 RPD (94.7%)          âš ï¸ CERCA DEL LÃMITE
```

## ğŸ”“ Actualizar a Plan de Pago (Futuro)

Si necesitas mÃ¡s capacidad:

1. **Azure OpenAI Service** (plan de pago)
   - Sin lÃ­mites estrictos de RPM/RPD
   - Pago por uso (pay-as-you-go)
   - Mejor para producciÃ³n

2. **Ollama Pro (cuando exista)**
   - Modelos mÃ¡s grandes localmente
   - Sin costos de API

3. **Hosting propio de modelos**
   - Llama 3, Mistral, etc.
   - Control total, sin lÃ­mites

## ğŸ“š Referencias

- [GitHub Models Docs](https://github.com/marketplace/models)
- [Azure AI Rate Limits](https://learn.microsoft.com/en-us/azure/ai-services/openai/quotas-limits)
- [Ollama Documentation](https://github.com/ollama/ollama)

## âœ… Estado Actual de Raymundo

| Feature | Estado |
|---------|--------|
| DetecciÃ³n de error 429 | âœ… Implementado |
| Fallback automÃ¡tico a Ollama | âœ… Implementado |
| NotificaciÃ³n al usuario | âœ… Implementado |
| Tracking de rate limits | âœ… Implementado (`/stats`) |
| CachÃ© de respuestas | âŒ Por implementar |
| Rate limiting preventivo | âŒ Por implementar |

---

**Ãšltima actualizaciÃ³n:** Febrero 6, 2026  
**VersiÃ³n:** 2.1
